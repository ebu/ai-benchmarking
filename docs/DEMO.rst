Demo: WER and normalization
============================

In this step-by-step demo you will compare the Word Error Rate (WER) of two transcripts. The WER is calculated against a less-than-perfect reference made from a human-generated subtitle file. You will also use normalization rules to improve the accuracy of the results.

To run this demo you will need a working installation of ``benchmarkstt`` and these source files saved to your working folder:

* `Subtitle file <https://github.com/ebu/benchmarkstt/resources/demos/qt_subs.xml>`_
* `Transcript generated by AWS <https://github.com/ebu/benchmarkstt/resources/demos/qt_aws.json>`_ 
* `Transcript generated by Kaldi <https://github.com/ebu/benchmarkstt/resources/demos/qt_kaldi.json>`_ 

This demo shows the capabilities of version 1 of the library, which benchmarks the accuracy of word recognition only. The library supports adding new metrics in future releases. Contributions are welcome. 

Make "reference"
----------------

Creating accurate verbatim transcripts for use as reference is time-consuming and expensive. As a quick and easy alternative, we will make a "reference" from a subtitle file. Subtitles are slightly edited and they include additional text like descriptions of sounds and actions, so they are not a verbatim transcription of the speech. Consequently, they are not suitable for calculating absolute WER. However, we are interested in calculating relative WER for illustration purposes only, so this use of subtitles is deemed acceptable. 

.. warning:: 

This is a demo of work in progress. The benchmarking tool is still in development and evaluations are not done for the purpose of assessing tools. In addition, the use of subtitles as reference will skew the results so they should not be taken as an indication of overall performance or as an endorsement of a particular vendor or engine.

We will use the subtitles file for the BBC's Question Time Brexit debate. This program was chosen for its length (90 minutes) and because live debates are challenging to transcribe.

The subtitles file is in XML format. Since the reference must be passed into :code:`benchmarkstt` as plain text, we need to strip out the XML tags. For this we will use the :code:`normalization` subcommand:  


.. code:: bash

  benchmarkstt normalization -i qt_subs.xml -o qt_reference.txt --regexreplace "<[^>]+>" " "

We used the normalization rule :code:`--regexreplace` that takes two parameters: a regular expression pattern and the replacement string. In this case all XML tags will be replaced with a space. This will result in a lot of space characters, but these are ignored by the diff algorithm so we don't have to clean then up.

The plain text file :code:`qt_reference.txt` was created. You can see that the XML tags are gone, but the file still contains non-dialogue text like 'APPLAUSE'. For better results you can manually clean up the text, or run the command again with a different normalization rule (not included in this demo). 

We now have a simple text file that will be used as the reference. The next step is to get the machine-generated transcripts for benchmarking.

Make hypotheses
----------------

Version 1 of :code:`benchmarkstt` does not integrate directly with STT vendors or engines, so transcripts for benchmarking have to be retrieved separately and converted to plain text. 

For this demo, two machine transcripts were retrieved for the Question Time audio: from AWS Transcribe and from the BBC's version of Kaldi, an open-source STT framework. This version was trained on BBC content so is expected to perform better. 

Both AWS and Kaldi return the transcript in JSON format, with word-level timings. They also contain a field with the entire transcript as a single string, and this is the value we will use (we don't benchmark timings in version 1). To make the hypothesis files, we simply copy the contents of the transcript field from both documents into two new text files: :code:`qt_aws_hypothesis.txt` and :code:`qt_kaldi_hypothesis.txt`.

Benchmark!
----------

We can now compare each of the hypothesis files to the reference in order to calculate the Word Error Rate. We process one file at a time, now using the :code:`metrics` subcommand, with two flags: :code:`--wer` is the metric to return; :code:`--diffcounts` returns the number of insertions, deletions, substitutions and correct words (the basis for WER calculation).


Calculate WER for AWS Transcribe:

.. code:: bash

  benchmarkstt metrics --reference qt_reference.txt --hypothesis qt_aws_hypothesis.txt --wer --diffcounts


Calculate WER for BBC-Kaldi:

.. code:: bash

  benchmarkstt metrics --reference qt_reference.txt --hypothesis qt_kaldi_hypothesis.txt --wer --diffcounts


After running these two commands, you can see that the WER for both transcripts is quite high (around 30%). Let's see the actual differences between the reference and the hypotheses by adding the :code:`--worddiffs` flag:

.. code:: bash

  benchmarkstt metrics --reference qt_reference.txt --hypothesis qt_kaldi_hypothesis.txt --wer --diffcounts --worddiffs

Normalize
---------

You can see that a lot of the differences are due to capitalization and punctuation. We are only interested in the correct identification of words, so we these types of differences should not count as errors. To get a more accurate WER, we will remove punctuations and convert all letters to lowercase. We will do this for the reference and both hypothesis files by using the :code:`normalize` subcommand again, with two rules: the built-in :code:`--lowercase` shortcut rule and the :code:`--regexreplace` rule:


.. code:: bash   

  benchmarkstt normalization -i qt_reference.txt -o qt_reference_normalized.txt --lowercase --regexreplace "[,.-]" " "

  benchmarkstt normalization -i qt_kaldi_hypothesis.txt -o qt_kaldi_hypothesis_normalized.txt --lowercase --regexreplace "[,.-]" " "

  benchmarkstt normalization -i qt_aws_hypothesis.txt -o qt_aws_hypothesis_normalized.txt --lowercase --regexreplace "[,.-]" " "

We now have normalized versions of the reference and the hypotheses. 

Benchmark again
---------------

Let's run the :code:`metrics` subcommand again, this time calculating WER based on the normalized files:

.. code:: bash

  benchmarkstt metrics --reference qt_reference_normalized.txt --hypothesis qt_aws_hypothesis_normalized.txt --wer --diffcounts --worddiff

  benchmarkstt metrics --reference qt_reference_normalized.txt --hypothesis qt_aws_hypothesis_normalized.txt --wer --diffcounts --worddiff

You can see that this time there are fewer differences between the reference and hypothesis. Accordingly, the WER is much lower for both hypotheses. The transcript with the lower WER is closer to the reference made from subtitles. 

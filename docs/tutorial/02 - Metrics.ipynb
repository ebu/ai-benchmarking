{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics\n",
    "\n",
    "Note: for a command line tool tutorial, see <https://benchmarkstt.readthedocs.io/en/latest/tutorial.html>\n",
    "\n",
    "Note: It is assumed that you have gone through the example in the Normalization tutorial first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created 3 normalizers, one for each file we intend to use in benchmark. The resulting code was:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarkstt.normalization import NormalizationComposite\n",
    "from benchmarkstt.normalization.core import Regex, Replace, Lowercase\n",
    "\n",
    "####### CONSTRUCT NORMALIZERS #######\n",
    "\n",
    "# aws\n",
    "normalizer_aws = NormalizationComposite()\n",
    "normalizer_aws.add(Regex('^.*\"transcript\":\"([^\"]+)\".*', '\\\\1'))\n",
    "normalizer_aws.add(Lowercase())\n",
    "                                  \n",
    "# kaldi\n",
    "normalizer_kaldi = NormalizationComposite()\n",
    "normalizer_kaldi.add(Regex('^.*\"text\":\"([^\"]+)\".*', '\\\\1'))\n",
    "normalizer_kaldi.add(Lowercase())\n",
    "\n",
    "# subtitles (reference)\n",
    "normalizer_ref = NormalizationComposite()\n",
    "normalizer_ref.add(Regex(\"</?[?!\\[\\]a-zA-Z][^>]*>\", \" \")) # Remove XML-tags\n",
    "normalizer_ref.add(Regex(\"[\\n\\s]+\", \" \")) # Remove extra newline and spaces\n",
    "\n",
    "# Remove non-dialogue text\n",
    "normalizer_ref.add(Replace('APPLAUSE', ''))\n",
    "normalizer_ref.add(Replace('SPEAKS OFF MIC', ''))\n",
    "normalizer_ref.add(Replace('INDISTINCT', ''))\n",
    "normalizer_ref.add(Replace('CHATTER FROM AUDIENCE', ''))\n",
    "normalizer_ref.add(Replace('LAUGHTER', ''))\n",
    "normalizer_ref.add(Replace('DROWNS OUT SPEECH', ''))\n",
    "normalizer_ref.add(Replace('GROANING', ''))\n",
    "normalizer_ref.add(Replace('CHEERING', ''))\n",
    "normalizer_ref.add(Lowercase())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the code for loading the filenames into variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can download these files at \n",
    "# https://github.com/ebu/benchmarkstt/tree/master/docs/_static/demos\n",
    "\n",
    "from os import path\n",
    "ROOT = \"../_static/demos\"\n",
    "\n",
    "####### LOADING THE FILES #######\n",
    "\n",
    "# Subtitle file\n",
    "subtitle_file = path.join(ROOT, \"qt_subs.xml\")\n",
    "\n",
    "# Transcript generated by AWS\n",
    "aws_transcript_file = path.join(ROOT, \"qt_aws.json\")\n",
    "\n",
    "# Transcript generated by Kaldi\n",
    "kaldi_transcript_file = path.join(ROOT, \"qt_kaldi.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Input classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use [`benchmarkstt.input.File`](https://benchmarkstt.readthedocs.io/en/latest/modules/benchmarkstt.input.core.html#benchmarkstt.input.core.File) to load the file contents, normalize it and split into segments.\n",
    "\n",
    "According to the documentation, input classes are \"responsible for dealing with input formats and converting them to benchmarkstt native schema\". This is the expected format used by the `compare` method of Metrics classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarkstt.input.core import File\n",
    "\n",
    "reference = File(\n",
    "    subtitle_file, \n",
    "    'plaintext', \n",
    "    normalizer_ref\n",
    ")\n",
    "\n",
    "hypothesis_aws = File(\n",
    "    aws_transcript_file, \n",
    "    'plaintext',\n",
    "    normalizer_aws\n",
    ")\n",
    "\n",
    "hypothesis_kaldi = File(\n",
    "    kaldi_transcript_file,\n",
    "    'plaintext',\n",
    "    normalizer_kaldi\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Word Error Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to calculate the metric 'WER' (= Word Error Rate). We can use the [`benchmarkstt.metrics.core.WER`](https://benchmarkstt.readthedocs.io/en/latest/modules/benchmarkstt.metrics.core.html#benchmarkstt.metrics.core.WER) class directly for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarkstt.metrics.core import WER\n",
    "wer = WER()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the WER for both ref/aws and ref/kaldi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS: 0.2987\n",
      "Kaldi: 0.3002\n"
     ]
    }
   ],
   "source": [
    "print(\"AWS: %.4f\" % wer.compare(reference, hypothesis_aws))\n",
    "print(\"Kaldi: %.4f\" % wer.compare(reference, hypothesis_kaldi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a Word Error Rate for both Kaldi and AWS and we can conclude in this single example, using the normalization rules as defined above, that Kaldi has a slightly higher Word Error Rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating DiffCounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [`benchmarkstt.metrics.core.DiffCounts`](https://benchmarkstt.readthedocs.io/en/latest/modules/benchmarkstt.metrics.core.html#benchmarkstt.metrics.core.DiffCounts), we can get some more details about the differences than with [`WER`](https://benchmarkstt.readthedocs.io/en/latest/modules/benchmarkstt.metrics.core.html#benchmarkstt.metrics.core.WER)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmarkstt.metrics.core import DiffCounts\n",
    "diffcounts = DiffCounts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS:\n",
      "OpcodeCounts(equal=11462, replace=2200, insert=682, delete=1710)\n",
      "Kaldi:\n",
      "OpcodeCounts(equal=11645, replace=2769, insert=888, delete=958)\n"
     ]
    }
   ],
   "source": [
    "print(\"AWS:\")\n",
    "print(diffcounts.compare(reference, hypothesis_aws))\n",
    "\n",
    "print(\"Kaldi:\")\n",
    "print(diffcounts.compare(reference, hypothesis_kaldi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show complete differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using [`benchmarkstt.metrics.core.WordDiffs`](https://benchmarkstt.readthedocs.io/en/latest/modules/benchmarkstt.metrics.core.html#benchmarkstt.metrics.core.WordDiffs), we can get the full diff between reference and hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS:\n",
      "\n",
      "Color key: Unchanged \u001b[31mReference\u001b[0m \u001b[32mHypothesis\u001b[0m\n",
      "\n",
      "\u001b[31m·bbc·2017·tonight,\u001b[0m\u001b[32m·tonight\u001b[0m·the·prime\u001b[31m·minister,\u001b[0m\u001b[32m·minister\u001b[0m·theresa·may,·the·leader·of·the·conservative\u001b[31m·party,·and\u001b[0m\u001b[32m·party·on·dh,\u001b[0m·the·leader·of\u001b[32m·the\u001b[0m·labour·party,·jeremy·corbyn,·face·\n",
      "\n",
      "\n",
      "Kaldi:\n",
      "Color key: Unchanged \u001b[31mReference\u001b[0m \u001b[32mHypothesis\u001b[0m\n",
      "\n",
      "\u001b[31m·bbc·2017·tonight,\u001b[0m\u001b[32m·tonight\u001b[0m·the·prime\u001b[31m·minister,\u001b[0m\u001b[32m·minister\u001b[0m·theresa\u001b[31m·may,\u001b[0m\u001b[32m·may\u001b[0m·the·leader·of·the·conservative\u001b[31m·party,\u001b[0m\u001b[32m·party\u001b[0m·and·the·leader·of\u001b[32m·the\u001b[0m·labour\u001b[31m·party,\u001b\n"
     ]
    }
   ],
   "source": [
    "from benchmarkstt.metrics.core import WordDiffs\n",
    "\n",
    "# we are using \"ansi\" diff dialect here, as it shows colored output,\n",
    "# making the output more human readable\n",
    "\n",
    "worddiffs = WordDiffs('ansi') \n",
    "\n",
    "print(\"AWS:\\n\")\n",
    "print(worddiffs.compare(reference, hypothesis_aws)[:300])\n",
    "\n",
    "print(\"\\n\\nKaldi:\")\n",
    "print(worddiffs.compare(reference, hypothesis_kaldi)[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that some of the differences are due to punctuation. Because in out case we are only interested in the correct identification of words, these types of differences should not count as errors. To get a more accurate WER, we will remove punctuation marks.\n",
    "\n",
    "We will do this for the reference and both hypothesis files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_punctuation = Regex('[,.-]', '')\n",
    "\n",
    "normalizer_ref.add(remove_punctuation)\n",
    "normalizer_aws.add(remove_punctuation)\n",
    "normalizer_kaldi.add(remove_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-check the WER and the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AWS WER: 0.2403\n",
      "Kaldi WER: 0.1978\n",
      "\n",
      "AWS diffs:\n",
      "\n",
      "Color key: Unchanged \u001b[31mReference\u001b[0m \u001b[32mHypothesis\u001b[0m\n",
      "\n",
      "\u001b[31m·bbc·2017\u001b[0m·tonight·the·prime·minister·theresa·may·the·leader·of·the·conservative·party\u001b[31m·and\u001b[0m\u001b[32m·on·dh\u001b[0m·the·leader·of\u001b[32m·the\u001b[0m·labour·party·jeremy·corbyn·face·the·voters·welcome·to·question·time·so·over·the·next\u001b[31m·90\u001b[0m\u001b[32m·ninety\u001b[0m·minutes·the·leaders\u001b[31m·of\u001b[0m\u001b[32m·off\u001b[0m·the·two·larger·parties·are·goin\n",
      "\n",
      "Kaldi diffs:\n",
      "\n",
      "Color key: Unchanged \u001b[31mReference\u001b[0m \u001b[32mHypothesis\u001b[0m\n",
      "\n",
      "\u001b[31m·bbc·2017\u001b[0m·tonight·the·prime·minister·theresa·may·the·leader·of·the·conservative·party·and·the·leader·of\u001b[32m·the\u001b[0m·labour·party·jeremy·corbyn·face·the·voters·welcome\u001b[31m·to·question·time\u001b[0m·so·over·the·next\u001b[31m·90\u001b[0m\u001b[32m·ninety\u001b[0m·minutes·the·leaders·of·the·two·larger·parties·are·going·to·be·quizzed·by·our·audience·here·\n"
     ]
    }
   ],
   "source": [
    "print(\"AWS WER: %.4f\" % wer.compare(reference, hypothesis_aws))\n",
    "print(\"Kaldi WER: %.4f\\n\" % wer.compare(reference, hypothesis_kaldi))\n",
    "\n",
    "print(\"AWS diffs:\\n\")\n",
    "print(worddiffs.compare(reference, hypothesis_aws)[:400])\n",
    "\n",
    "print(\"\\nKaldi diffs:\\n\")\n",
    "print(worddiffs.compare(reference, hypothesis_kaldi)[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, both AWS and Kaldi show a significant drop in WER and the WordDiffs seem more like what we would expect.\n",
    "\n",
    "It is left as an exercise to the reader to further extend the normalizers to get even more representative WERs (e.g. by adding a `Replace(\"ninety\", \"90\")` to the hypothesis normalizers, removing the \"bbc 2017\" at the start, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
